---
title: "hello, universe!"
date: 2025-07-10
categories: ai mathematics formalization
permalink: /posts/2025/07/blog-post-1/
tags:
  - mathematics
  - ai
  - lean
  - predictions
---
```
tldr; this is a re-post with a twist. read more to find out how closely the AI approximated what I wanted to write.
```

With this new AI hype, I couldn't pass the chance to finally get back to writing and self-studying mathematics. It wasn't AI by itself that got me interested again. It was a programming language called `lean` (`lean4` at the time of writing). I'm no stranger to programming and have used AI to accelerate prototype code to understand a project better (constraints, requirements, the structure of the interfaces, and implementation details). Mathematicians have a similar thing where they have proof ideas. A proof idea has the seeds. You work on the idea until it bears fruit. Then you can share it in academic journals and it may become a seed for another proof. If the final proof is complicated, it probably looks very different from the proof idea.

I tried my hand at this after graduating with an undergraduate degree in Mathematics and Computer Science. I would spend time after work reading undergraduate texts and doing my own research. I kept revisiting my proof ideas and would have a good grasp on the techniques used which helped to also deepen my understanding. I didn't have a TA or professor to grade my work so I was my own proof checker. I took an Inquiry-Based Learning (IBL) number theory course in college that gave me the confidence to do this on my own. That class encouraged independent critical thinking and communicating ideas to others in the class and collaboration. I don't think I really encountered anything that required a complex proof. I had a feeling that my proofs or proof ideas were correct or would lead to the right direction. This was textbook undergraduate math after all.

But with `lean`, I can formalize my proofs. I just had to learn the language and translate my proofs. With this, I have guard rails that I can use to keep going on my journey into more complicated areas. With the advent of AI, it becomes even more powerful. It could suggest hints, examples, resources to learn more, or different perspectives. Knowing that I'm going in the right direction (and when I'm not) coupled with the thought that I'll eventually make progress (as long as I put in the work) is a great feeling. I'm in no pressure to get the "right" answer. It's simply because of my own curiosity. This got me thinking about how the future of mathematics might be taught.

> Mathematics teaching will become more abstract - emphasizing critical thinking, reasoning, creativity, and experimentation

Today we live in a world where abstract mathematics is not taught until much later in high school or college. I don't think there is anything wrong with that per se. We have an aging education system with limited resources. However, with some sort of socratic AI that can fact check itself, I think it will be feasible to revise the curiculum so that abstract mathematics can be offered much earlier (perhaps middle school). Imagine peering into this classroom. Students have a persoonalized tutor that can engage them and help them focus on things they want to understand about the subject. I think the material will be simple enough where no formalization will be needed and students at this level can just use natural language with pencil and paper. For students that show a liking to the subject, there can be research seminars or classes where formalization is introduced. I imagine a student spending their summer doing this seminar where exploration and experimentation are encouraged. They have the guard rails of formalization and a personalized tutor to help them if they get too stuck. 

In the best case, this could lead to students contributing ideas in mathematics very early on in their academic life. Possibly to an open source mathematics formalization project with other students from around the world. Imagine middle schools where kids come up with their own conjectures, share them with the class, and come up with counterexamples or proofs in standard topics such as euclidean geometry, number theory, combinatorics, or linear algebra. It won't matter if they re-prove an existing result though, the emphasis is on learning the mechanics of abstract mathematical research. This can be carried over to see high school as students learn and are able to reason in a broader world exploring calculus and so on. I imagine graduate schools where students build whole new theories (perhaps ones where the seeds were planted by their own curiosity in earlier schooling!) on top of existing ones or modernizing them - kind of like mini-Bourbakis or Shinichi Mochizuki. However, in this future, there will no longer be doubt about the foundations that the theory is built on. The formalization at this stage will be necessary and with the help of AI, it can be somewhat automated. Of course, professors will still mentor their students and collaborate with them.

In all these cases, the journey is still there but the destination is further than before. The journey is what it's all about - and where the learning lies. In the background of all of this of course will be `lean` or some computable way to compile a proof for correctness as well as an AI partner to help the student through a personalized socratic method. 

This line of thinking got me thinking about over-reliance on these kinds of technologies. My worry is two-fold:
1. The worry is that it weakens our ability to think (learned helplessness). 
2. Next, what if there is a bug in the formalization software? Or what if the AI is not helpful?

Let's tackle the first worry.

> "Whose thoughts am I using?"

Does this content sound familiar somehow? I have a confession. It got late when writing my previous post so I used Claude AI to finish it. I gave it my first draft as a template. I didn't read what Claude generated though, I just blindly committed it as the first post. I wanted to compare what Claude generated versus my publication of the first draft. Why do that? to see how close Claude got (on it's first attempt) to what I wanted to express. Here's the chat history I used to generate the first post and the follow up that got me thinking further:  https://claude.ai/share/dcaf4663-f99e-4987-bd1a-e84758e5d460. 

> I suggest trying this exercise yourself. 

Over time, AI could have more data regarding my writing style and should approximate closer to what I want on the first try. The process might be something like:

1. Start first draft
2. Commit to main branch
3. Use Claude to finish with first draft as template
4. Finish the first draft on your own in another topic branch
5. Review the artifact
   a. Suggest high level changes (get lucky) OR
   b. Implement the changes myself in step 4 using a PR and use the diff as suggestions
6. Iterate until satisfied
7. Commit back to main branch

However, it could get to the point where it can write a post in my likeness. Now, of course, this is github. I could have just made this post a new commit on top of my previous post and did a diff. The diff would highlight the differences between this post and the previous one line by line. I can't yet decide at this stage what I want this to be. It could be more like a journal (which is what I was initially thinking) or it could be more like a collection of living mini essays. It will probably be some hybrid of the two. 

Anyway, confession over. Let's continue on the first worry:

> I believe there is a deep human need to be authentic. 

In my vision for the future, I don't address cheaters. Cheating is already a huge problem today. I dislike cheating. In college, I was baffled by students that would cheat. Why pay so much money not to do the work and learn? It's like paying your physical instructor to train someone else instead of yourself. Why not spend that money on something else then? I imagine that if someone in this world cheated, it would be a waste of their own time - especially the higher they get in school. Perhaps they are so disinterested in the subject that they resort to cheating. Perhaps the subject in question is some required part of the curriculum. Then maybe the student has a point - should the class really be required? I believe there is still value in learning a subject that may not seem interesting at first. Learning is a valuable skill in itself and being able to use it even when you don't want to is valuable in the real world. Learn for learning's sake. It's like cross-training in basketball even though you're a boxer. Or like learning to paint even though you're a musician. You could still learn something valuable that translates to your main interest. At first, it may not seem like there's a connection but maybe you have to think about it differently. A core skill in mathematics after all is finding connections between disparate things. It could be a key to unlocking a fruitful path less traveled in your main interest.

If one is curious about learning a subject, they aren't going to be satisfied with just any answer. They're going to want to understand the answer - why is it true? is there an easier way? or a more elegant way? In my IBL number theory class, there was a policy on collaboration in the syllabus. We could collaborate to share ideas but not copy each other's work. In practice, the high level ideas were discussed and the steps to prove them were often left out (there was a lot of creative freedom so people had different approaches here anyway). If proofs were discussed, it was often a high level proof idea on how to start. We were often tested on connecting these high level proof ideas to do something different. I found this approach effective in testing understanding. Copying solutions or even following proofs in the books without making the connections between ideas by applying them in homework problems wouldn't be enough to find the answer. It was not a simple thing, there was an understanding of the problem that had to be wrestled with that gave the constraints for the answer. An understanding of the deeper idea had to be recognized to apply it in a different context.

This problem generalizes to anyone that generates content - writers, programmers, musicians, artists, etc.; To address the problem of cheating, one may just keep a record of the dialogue with the AI to see if there is a prompt to generate the content. This is weak evidence though. I could have linked a slightly altered converstaion (using less specific or demanding prompts) to give the impression that I contributed more ideas to this post instead of the AI. Perhaps I hid the real link that is just the content of this post (this is really me typing ðŸ¤ž). If someone links an AI conversation - they want you to think that's the conversation they had. Whether they are honest in their process or they are using the linked AI conversation as part of their narrative to get you to believe something - how can we ever know? 

Cheating has always been around. There are ghostwriters, record label industry plants, etc. When money is involved, the problem becomes worse. In a future where one cannot prove whether I am actually typing these posts, how can we trust? From the perspective of the cheater, it does seem easier to have AI think for you. It can be easier to edit than to create something from scratch. In a world where $P \neq NP$, verifying an answer is easier than devising an algorithm to generate an answer. The former is a computation, the latter is somehow more. 

I think the solution may come from psychology. Prevention is the best medicine. The mindset here matters. Would you be happy living comfortably in the matrix or enduring a real life? In the matrix, your achievements are not real and I think that is deeply unsettling to us humans. There is a gap between how you perceive yourself and how others perceive you. I think there are limits to how wide this gap can become before it starts to deteriorate one's mental health. I believe there is a deep human need to be authentic. To be seen as they truly are: "The truth shall set you free". If one is authentic, naturally, it will show in the history of contributions/work, the intention(s) of the authors, the veracity of claims, and the respect from others in the community. Otherwise, if one cheats, they will have to work harder (bribe, coerce, etc.) to score well on these signals. 

How much of the content was from you and how much was it from the AI? I'm curious to see how this will unfold in the future. I doubt AI companies would expose users history to the public but perhaps they will for some high profile legal case. Perhaps people get so good at being trained on AI content that they learn to spot it. Like looking for those glitches in the Matrix. Hopefully one day there will be some way to trace the lineage of ideas from a piece of content. This would be beneficial for critical thinking in general so that one can see the flow of ideas over time but I digress. I think the trace structure of honest collaboration would look very different from cheating.

Now, onto the second worry:

> "The Spectre of Error"

AI has a positivism about it that makes it seem like anything can be accomplished. In mathematics, this kind of wishful thinking is tempered by low expectations and patience. It's great when things work out. However, when they don't it can feel like going back to square one. AI is known to hallucinate. This implies that it does not reason. Some hallucinations are easy to tell. In fact, these can be valuable as one strategy in problem solving is to just try something. But what if the hallucinations are hard to tell? In a mathematical research context, where there probably isn't a lot of data at the cutting edge, they can still suggest directions to take instead of saying "I don't know". 

Over time, mathematicians develop an instinct for which path they should choose next. The danger is that the AI could subtly be leading you down a path that seems promising but is ultimately a dead end. A dead end that could take years to arrive at. In some way, this is still valuable - the field learns from it's failures as much as it's successes. But is this the path you would have taken? This ties in to learned helplessness - will mathematicians still have this intuition for choosing a feasible path if they just rely on AI to enumerate the paths for them?

There's nothing about AI that makes this problem unique to it. Mathematicians have been led to false paths in their research before adjusting. Having an instinct to know when to choose a different path seems harder to develop to me. Just because there's no evidence does not necessarily mean it won't work. It could be that the path is just less traveled and wouldn't it be romantic to publish a positive result along with what seemed like a fruitless journey!? Mathematicians will still have to tread carefully on paths they pursue. They will still have to keep an open mind and not be afraid to try different paths at once. 

And then there's the formalization software (it's dependencies all the way down to hardware). `lean` could have bugs. In systems with ever growing complexity, this could have far reaching consequences. Especially since it plays a central gate-keeping role in the community. Software fails constantly. The road of software bugs is paved with the best intentions - and simple accidents. Hardware is better tested but cosmic rays could flip a bit in some hardware causing a proof to compile when it shouldn't have (false positive). Or it could have rejected a valid proof (false negative). Ok - this is unlikely but in the world of mathematics, we deal with absolutes and guarantees.

So what happens in this world when the formalization software has a bug? Without formalization, mathematics can fall back to chalk & chalkboards. The community might take a productivity hit in research but it should still make progress. If formalization software had a bug, then proofs would have to be updated if they are impacted. This could have a chain effect where updated proofs reveal other broken proofs. This sort of existential crisis could halt research indefinitely. Science journals would write about it and hopefully how it all came to be resolved happily. I can see the commit message for the fix now: "fixes mathematics as we know it". I suspect "mathematicians" (more likely engineers) that focused too much on the mechanics of the formalization language rather than the proof itself would be vulnerable to this.

Though unlikely, imagine an instance where researchers are so convinced of their proof but the formalization software still does not accept it. It shows them where their logic fails. The researchers argue that this is a bug in the software. A contentious debate erupts. Who do you believe? the compiler? or the humans? This could concern the formalization of a new theory - or even more interesting, a formalization of an old theory we thought was true. If the compiler is indeed correct, this will likely lead to a refinement in the theory where some ambiguity is made explicit and then addressed with some new definitions or axioms. If the humans are correct, the bug could shed light on the limits of the formalization software language (Godel's Incompleteness says so after all). Either way, these are valuable finds.

Mathematicians should not blindly trust the formalization software. We should not revere it as "all knowing" and "how dare we blaspheme against it!". We revisit old proofs and theories today to check for their correctness, we may hesitate to do this in this new world because the compiler says it is correct. But what if it is wrong? We should not blindly trust. Again, nothing new here for the mathematician. The skills they have gain even more importance in this new world.


> Scout's honor

Worry not, dear reader. I value my own thoughts and this post is organic. My intention is to express myself after all - it's my blog. Of course, I learn over time and perhaps the views I have may change. My predictions could be wildly wrong and if they interest me, I'll write about those too. It's the journey after all. The reflective process that separate us from AI.

If you read this far, I am grateful. I hope it got you thinking of the future as well.
