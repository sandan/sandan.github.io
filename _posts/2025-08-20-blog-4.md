---
title: 'Propositions: Part 2'
date: 2025-08-20
permalink: /posts/2025/08/blog-post-4/
tags:
  - lean
  - logic
  - propositions
  - disjunction
---

In the previous chapter, we explored propositions and conjunctions. Recall that propositions are statements that are either true or false, while conjunctions are compound statements that join two or more propositions with "and" (symbolized as `∧`).

This chapter introduces **disjunctions** — compound statements that join propositions with "or" (symbolized as `∨`). When we have propositions `p` and `q`, the disjunction `p ∨ q` means "p or q (or both)."

## Examples of Disjunctions

Consider these mathematical statements:

- "An integer is positive, negative, or zero"
- "A natural number is prime or composite"  
- "An integer is even or odd"
- "An element x is in set A or is in set B"

## Inclusive vs. Exclusive Disjunctions

**Inclusive disjunctions** assert that *at least one* of the propositions is true. The statement `p ∨ q` means that `p` is true, `q` is true, or both are true. This is the standard mathematical interpretation of "or."

However, some disjunctions appear to suggest that exactly one proposition is true — that `p` and `q` cannot both hold simultaneously. When propositions are **mutually exclusive**, we have `p ∨ q` but never `p ∧ q`.

## A Closer Look at Our Examples

Interestingly, our examples demonstrate both types:

- "An integer is positive, negative, or zero" — These are mutually exclusive categories
- "An element x is in set A or is in set B" — This allows for x to be in both sets (inclusive)

Some of the most fascinating propositions in mathematics are disjunctions that *appear* mutually exclusive but are actually inclusive, or vice versa. Understanding this distinction becomes crucial as mathematical statements grow more complex.

We'll explore exclusive disjunctions more thoroughly in a future chapter after we develop the tools of negation. For now, remember that mathematical "or" is inclusive by default — it's the "at least one" interpretation that opens doors to richer logical reasoning.

Let's look at an example in Lean:
``` lean
example (p q : Prop) (h : q) : p ∨ q := by
  -- h : q
  -- ⊢ p ∨ q
  right
  -- h : q
  -- ⊢ q
  exact h
```
If we have a disjunction as our goal, we have to prove at least one of the propositions are true. We can use the `left` and `right` tactics to choose which proposition to choose. In the proof above, we use `right` since we have a hypothesis giving us a proof for `q`.

With the previous example in mind, try this example yourself:
``` lean
-- Let p and q be propositions. Suppose we know p. Prove p or q.
example (p q : Prop) (h : p) : p ∨ q := by
  sorry
```

Similar to conjunctions `P ∨ Q ∨ R` associates like so `P ∨ (Q ∨ R)`:
```lean
example (p q r : Prop) : (p ∨ q) ∨ r ↔ p ∨ q ∨ r := by
  exact or_assoc
```
Another way to observe this in Lean, is to see how `left` and `right` tactics behave in an expression like this:
``` lean
example (p q r : Prop) (h : r) : p ∨ q ∨ r := by
  -- h : r
  -- ⊢ p ∨ q ∨ r
  right
  -- ⊢ q ∨ r
  right
  -- ⊢ r
  assumption
```

Now try this example yourself. It should be shorter than the previous proof:
``` lean
example (p q r : Prop) (h : r) : (p ∨ q) ∨ r := by
  sorry
```

## Working with Disjunctions in Proofs

When a disjunction appears in our hypotheses, we need a different approach than we used with conjunctions. Since we don't know *which* proposition in the disjunction is true, we must prove our goal holds in *every possible case*.

The `cases with` tactic handles this. Given a disjunction hypothesis `h : p ∨ q`, this tactic splits our proof into two separate cases:

- **Case `inl`**: We assume `p` is true and prove the goal
- **Case `inr`**: We assume `q` is true and prove the goal

### Key Differences from Conjunctions

Unlike conjunctions, where `cases` simply deconstructed both propositions simultaneously, disjunctions require case-by-case analysis:

- **With conjunctions**: We knew both propositions were true, so we could extract them as separate hypotheses
- **With disjunctions**: We only know that *at least one* proposition is true, so we must consider each possibility individually

The syntax uses pattern matching with the pipe symbol (`|`) to specify distinct cases, where `inl` represents the left proposition and `inr` represents the right proposition in the disjunction.

This case analysis approach ensures our proofs are complete — by proving the goal holds whether `p` is true or `q` is true, we've covered all possibilities that make `p ∨ q` true.

In this next example, we know `p or q`. We know `p`. We know `q`. Therefore, we can make a stronger claim that `p` is true and `q` is true.
``` lean
example (p q : Prop) (hp : p) (hq : q) (hpq : p ∨ q) : p ∧ q := by
  cases hpq with   -- Let's suppose p or q
  | inl h1 =>      -- Suppose that p is true. Label that hypothesis h1.
    -- h1 : p
    constructor    -- show p ∧ q
    · -- 
      exact h1     -- we could have also used hp here
    · exact hq
  | inr h2 =>      -- Now, let's suppose q is true. Label that hypothesis h2.
    -- h2 : q
    constructor    -- show p ∧ q
    · exact hp
    · exact h2     -- we could have also used hq here
```
(Note: This is a contrived example since we already have hp and hq directly!)

## Commutativity of Disjunctions
Disjunctions are commutative, just like conjunctions:
``` lean
example (p q : Prop) (h : p ∨ q) : q ∨ p := by
  cases h with    -- Let's suppose p or q
  | inl hp =>     -- Suppose p is true and label that hypothesis hp.
                  -- ⊢ q ∨ p
    right         -- ⊢ p
    exact hp
  | inr hq =>     -- Now, let's suppose q is true and label that hypothesis hq.
                  -- ⊢ q ∨ p
    left          -- ⊢ q
    exact hq
```
This fact is already in Lean. We can use `or_comm`:
```lean
example (p q : Prop) (h : p ∨ q) : q ∨ p := by
  rw[or_comm]
  -- ⊢ p ∨ q
  exact h
```

## Combining Conjunctions and Disjunctions
Now that we understand both conjunctions and disjunctions, let's explore how they interact in compound propositions.

### Operator Precedence
The "and" operator (`∧`) binds more tightly than "or" (`∨`). This means `p ∧ q ∨ r` is interpreted as `(p ∧ q) ∨ r`:
```lean
-- ∧ (and) has precedence over ∨ (or)
example (p q r : Prop) (hp : p) (hq : q) : p ∧ q ∨ r := by
  left
  -- ⊢ p ∧ q
  constructor
  · -- ⊢ p
    exact hp
  · -- ⊢ q
    exact hq

```
You can always use parentheses to modify the default precedence. Try proving this next:
``` lean
example (p q r : Prop) (hp : p) (hq : q) : p ∧ (q ∨ r) := by
  constructor
  -- case left
  -- p q r : Prop
  -- hp : p
  -- hq : q
  -- ⊢ p
  · sorry

  -- case right
  -- p q r : Prop
  -- hp : p
  -- hq : q
  -- ⊢ q ∨ r
  · sorry

```

## The Power of Case Analysis: Distributive Laws
Let's prove a distributive law: `p ∧ (q ∨ r) → (p ∧ q) ∨ (p ∧ r)`. Our first attempt might get stuck:
``` lean
example (p q r : Prop) (h : p ∧ (q ∨ r)) : p ∧ q ∨ p ∧ r := by
  -- ⊢ p ∨ q ∨ p ∧ r
  left                 -- Let's try to prove the left proposition
  -- ⊢ p ∧ q
  constructor
  · exact h.1          -- Show p
  · sorry              -- Show q
                       -- at this point, all we have is:
    -- p q r : Prop
    -- h : p ∧ (q ∨ r)
    -- ⊢ q
                       -- we need more hypotheses to conclude q
```
When we try to show `q`, there isn't any hypothesis we can seemingly use directly. If we try to start with `right`, we'll get stuck again trying to show `r`. The key insight is to use case analysis.

Often, to complete a proof, the key is to split the problem into cases. There is a lot of creativity involved in choosing how to decompose the problem into cases. It's like origami. There are many ways to make the same shape from a piece of paper. The seams that you can fold are like choosing the cases to split on that will eventually make the right shape (the goal).

### Introducing the by_cases Tactic
The `by_cases` tactic splits our proof based on whether a proposition is true or false. Here's a simple example:
```lean
example (q : Prop) : q ∨ ¬q := by
  by_cases hq : q  -- Suppose q and label that hypothesis hq
  · -- hq : q
    -- ⊢ q ∨ ¬q
    left           -- We show q
    exact hq
  · -- hq : ¬q     -- Now suppose not q and label that hypothesis hq
    -- ⊢ q ∨ ¬q
    right          -- we show not q
    exact hq
```
The proof required us to look at a new symbol called negation (`¬`). This is a unary operator. Unlike the `∧` and `∨` operators, negation just requires a single proposition. We'll look more at negation and how it relates to conjunctions and disjunctions in the next chapter.

### Applying Case Analysis to Our Distributive Law
With this new tactic, we can try tackling the problem again. What should we use for the hypothesis in the `by_cases` tactic? We can easily deduce `p` and use it. What about `q`?
``` lean
example (p q r : Prop) (h : p ∧ (q ∨ r)) : p ∧ q ∨ p ∧ r := by
  by_cases hq : q
  ·                       -- Suppose q is true and label that hypothesis hq
    -- hq : q
    -- ⊢ p ∧ q ∨ p ∧ r
    left                  -- We show p ∧ q 
    constructor
    · -- ⊢ p
      exact h.1           -- Showing p is easy. That's given by our hypothesis h.
    · -- ⊢ q
      exact hq            -- This is where we were stuck on our previous proof. But now,
                          -- showing q is easy, too. That's given by hq from the by_cases tactic.

  ·                       -- Now suppose q is not true (negation of hq) symbolized by ¬q.
                          -- ¬q means "the opposite truth value of q". If q is true, ¬q is false.
                          -- Note: We'll go over negation (¬) in more detail in the next chapter.

    -- hq : ¬q
    -- ⊢ p ∧ q ∨ p ∧ r
    right                 -- We show p ∧ r
    constructor
     -- h : p ∧ (q ∨ r)
     -- hq : ¬q
     -- ⊢ p
    · exact h.1           -- Showing p is easy. That's given by h.
    ·                     -- Now the goal is to show r.
                          -- At this point, we have:
      -- h : p ∧ (q ∨ r)
      -- hq : ¬q          
      -- ⊢ r
                           -- If we look at h.2 (q ∨ r) and hq (¬q), it seems logical to deduce r.
                           -- After all, either q is true or r is true and we know q is false.
                           -- h.2 is a disjunction. We've dealt with these before.
      cases h.2 with       -- We consider cases for q ∨ r.
      | inl hq' =>         -- Now suppose q and label the hypothesis hq'. At this point, we have:

      -- hq : ¬q
      -- hq' : q
      -- ⊢ r
        contradiction      -- The contradiction tactic can be used to close any goal
                           -- when we have hypotheses that are opposed to each other (or False).
                           -- In this case, we have hq and hq'. These are directly opposed.
                           -- in classical logic, this is known as the principle of exposion.
                           -- Also known as ex falso quodlibet in latin, which means 
                           -- "from falsehood, anything follows".
                           -- In particular, "anything" refers to our goal in this case (r).

      | inr hr =>          -- Lastly, suppose r and label that hypothesis hr
        -- hr : r
        -- ⊢ r
        exact hr           -- Showing r is easy, we have hr.
```
The key insight: when we know `q ∨ r` is true but `q` is false, then `r` must be true!

The proofs above required looking at another new tactic called `contradiction`. We'll look more into how this tactic can be used in a future chapter alongside negation.

Now, could we have also split on proposition `r`? Try this one yourself:
```lean
example (p q r : Prop) (h : p ∧ (q ∨ r)) : p ∧ q ∨ p ∧ r := by
  by_cases hr : r
  · -- case pos
    -- p q r : Prop
    -- h : p ∧ (q ∨ r)
    -- hr : r
    -- ⊢ p ∧ q ∨ p ∧ r
    right
    sorry
  · -- case neg
    -- p q r : Prop
    -- h : p ∧ (q ∨ r)
    -- hr : ¬r
    -- ⊢ p ∧ q ∨ p ∧ r
    left
    sorry
```
This case analysis approach is fundamental to working with compound logical statements. By carefully choosing which proposition to split on, we can transform seemingly impossible proofs into straightforward ones.