---
title: 'Propositions: Part 2'
date: 2025-08-20
permalink: /posts/2025/08/blog-post-4/
tags:
  - lean
  - logic
  - propositions
  - disjunction
---

# Disjunctions

In the previous chapter, we explored propositions and conjunctions. Recall that propositions are statements that are either true or false, while conjunctions are compound statements that join two or more propositions with "and" (symbolized as `∧`).

This chapter introduces **disjunctions** — compound statements that join propositions with "or" (symbolized as `∨`). When we have propositions `p` and `q`, the disjunction `p ∨ q` means "p or q (or both)."

## Examples of Disjunctions

Consider these mathematical statements:

- "An integer is positive, negative, or zero"
- "A natural number is prime or composite"  
- "An integer is even or odd"
- "An element x is in set A or is in set B"

These examples reveal something important: disjunctions can behave differently depending on context.

## Inclusive vs. Exclusive Disjunctions

**Inclusive disjunctions** assert that *at least one* of the propositions is true. The statement `p ∨ q` means that `p` is true, `q` is true, or both are true. This is the standard mathematical interpretation of "or."

However, some disjunctions appear to suggest that exactly one proposition is true — that `p` and `q` cannot both hold simultaneously. When propositions are **mutually exclusive**, we have `p ∨ q` but never `p ∧ q`.

## A Closer Look at Our Examples

Interestingly, our examples demonstrate both types:

- "An integer is positive, negative, or zero" — These are mutually exclusive categories
- "An element x is in set A or is in set B" — This allows for x to be in both sets (inclusive)

Some of the most fascinating propositions in mathematics are disjunctions that *appear* mutually exclusive but are actually inclusive, or vice versa. Understanding this distinction becomes crucial as mathematical statements grow more complex.

We'll explore exclusive disjunctions more thoroughly in a future chapter after we develop the tools of negation. For now, remember that mathematical "or" is inclusive by default — it's the "at least one" interpretation that opens doors to richer logical reasoning.

Let's look at an example in Lean:
``` lean
example (p q : Prop) (h : q) : p ∨ q := by
  -- h : q
  -- ⊢ p ∨ q
  right
  -- h : q
  -- ⊢ q
  exact h
```
If we have a disjunction as our goal, we have to prove at least one of the propositions are true. We can use the `left` and `right` tactics to choose which proposition to choose. In the proof above, we use `right` since we have a hypothesis giving us a proof for `q`.

With the previous example in mind, try this example yourself:
``` lean
example (p q : Prop) (h : p) : p ∨ q := by
  sorry
```

Similar to conjunctions `P ∨ Q ∨ R` associates like so `P ∨ (Q ∨ R)`.
``` lean
example (p q r : Prop) (h : r) : p ∨ q ∨ r := by
  -- h : r
  -- ⊢ p ∨ q ∨ r
  right
  -- ⊢ q ∨ r
  right
  -- ⊢ r
  assumption
```

Now try this example yourself. It should be shorter than the previous proof:
``` lean
example (p q r : Prop) (h : r) : (p ∨ q) ∨ r := by
  sorry
```

We can also have disjunctions in our hypotheses. To use it, we consider each of the propositions in the disjunction and prove the goal in each case. The `cases with` tactic allows you to do just that. Each case has the same goal but you you have a new hypothesis to work with. The `cases` tactic was also used with conjunctions. We saw how it could be used to deconstruct the propositions into named hypotheses. 

For a disjunction, we can't just deconstruct the propositions. If we have a disjunction, we don't know which of the propositions in it could be true. The `cases with` tactic used on a disjunction hypothesis `h : p ∨ q` works a little differently. You can deconstruct only one of the propositions at a time. There are two different named cases `inl` and `inr` that take hypothesis labels. These cases are syntactically implemented using pattern matching (the pipe symbol specifies distinct cases).

In this next example, we know `p or q`. We know `p`. We know `q`. Therefore, we can make a stronger claim that `p` is true and `q` is true.
``` lean
example (p q : Prop) (hp : p) (hq : q) (hpq : p ∨ q) : p ∧ q := by
  cases hpq with   -- Let's suppose p or q
  | inl h1 =>      -- Suppose that p is true. Label that hypothesis h1.
    -- h1 : p
    constructor    -- show p ∧ q
    · -- 
      exact h1     -- we could have also used hp here
    · exact hq
  | inr h2 =>      -- Now, let's suppose q is true. Label that hypothesis h2.
    -- h2 : 
    constructor    -- show p ∧ q
    · exact hp
    · exact h2     -- we could have also used hq here
```
Admittedly, this is a contrived example because we could easily deduce the conclusion with `hp` and `hq`. The hypothesis `hpq` is not necessary.

Let's look at another property of disjunctions. Disjunctions are commutative just like conjunctions.
``` lean
example (p q : Prop) (h : p ∨ q) : q ∨ p := by
  cases h with    -- Let's suppose p or q
  | inl hp =>     -- Suppose p is true and label that hypothesis hp.
                  -- ⊢ q ∨ p
    right         -- ⊢ p
    exact hp
  | inr hq =>     -- Now, let's suppo"se q is true and label that hypothesis hq.
                  -- ⊢ q ∨ p
    left          -- ⊢ q
    exact hq
```
Now that we know how to handle conjunctions and disjunctions, let's work on compound propositions combining both of these operators.

First, we look at the precedence of operators. The "and" operator binds propositions more tightly than "or". 
```lean
-- ∧ (and) has precedence over ∨ (or)
example (p q r : Prop) (hp : p) (hq : q) : p ∧ q ∨ r := by
  left
  -- ⊢ p ∧ q
  constructor
  · -- ⊢ p
    exact hp
  · -- ⊢ q
    exact hq

```
You can always use parentheses to modify the default precedence of operators. Try proving this next:
``` lean
example (p q r : Prop) (hp : p) (hq : q) : p ∧ (q ∨ r) := by
  constructor
  -- case left
  -- p q r : Prop
  -- hp : p
  -- hq : q
  -- ⊢ p
  · sorry

  -- case right
  -- p q r : Prop
  -- hp : p
  -- hq : q
  -- ⊢ q ∨ r
  · sorry

```

 The next example proves a distributive rule of `∧` (and) over `∨` (or). We might try to start like this:
``` lean
example (p q r : Prop) (h : p ∧ (q ∨ r)) : p ∧ q ∨ p ∧ r := by
  -- ⊢ p ∨ q ∨ p ∧ r
  left                 -- Let's try to prove the left proposition
  -- ⊢ p ∧ q
  constructor
  · exact h.1          -- Show p
  · sorry              -- Show q
                       -- at this point, all we have is:
    -- p q r : Prop
    -- h : p ∧ (q ∨ r)
    -- ⊢ q
                       -- we need more hypotheses to conclude q
```
When we try to show `q`, there isn't any hypothesis we can seemingly use. If we try to start with `right`, we'll get stuck again trying to show `r`. Let's try another approach.

Often, to complete a proof, the key is to split the problem into cases. The fun part is choosing how to decompose the problem into cases. It's like origami. The problem is a piece of paper. You look for seams that you can fold so that we can make the right shape (the goal).

For proving compound propositions, the seams naturally split on whether a proposition is true or false. The `by_cases` tactic can be used to split on a given hypothesis. The goal is then proven in the context of each case. Let's look at a simple example:
```lean
example (q : Prop) : q ∨ ¬q := by
  by_cases hq : q  -- Suppose q and label that hypothesis hq
  · -- hq : q
    -- ⊢ q ∨ ¬q
    left           -- We show q
    exact hq
  · -- hq : ¬q     -- Now suppose not q and label that hypothesis hq
    -- ⊢ q ∨ ¬q
    right          -- we show not q
    exact hq
```
The proof required us to look at a new symbol called negation (`¬`). This is a unary operator. Unlike the `∧` and `∨` operators, negation just requires a single proposition. We'll look more at negation and how it relates to conjunctions and disjunctions.

With this new tactic, we can try tackling the problem again. What should we use for the hypothesis in the `by_cases` tactic? We can easily deduce `p` and use it. What about `q`?
``` lean
example (p q r : Prop) (h : p ∧ (q ∨ r)) : p ∧ q ∨ p ∧ r := by
  by_cases hq : q
  ·                       -- Suppose q is true and label that hypothesis hq
    -- hq : q
    -- ⊢ p ∧ q ∨ p ∧ r
    left                  -- We show p ∧ q 
    constructor
    · -- ⊢ p
      exact h.1           -- Showing p is easy. That's given by our hypothesis h.
    · -- ⊢ q
      exact hq            -- This is where we were stuck on our previous proof. But now,
                          -- showing q is easy, too. That's given by hq from the by_cases tactic.

  ·                       -- Now suppose q is not true (negation of hq) symbolized by ¬q.
                          -- ¬q means "the opposite truth value of q". If q is true, ¬q is false.
                          -- Note: We'll go over negation (¬) in more detail in the next chapter.

    -- hq : ¬q
    -- ⊢ p ∧ q ∨ p ∧ r
    right                 -- We show p ∧ r
    constructor
     -- h : p ∧ (q ∨ r)
     -- hq : ¬q
     -- ⊢ p
    · exact h.1           -- Showing p is easy. That's given by h.
    ·                     -- Now the goal is to show r.
                          -- At this point, we have:
      -- h : p ∧ (q ∨ r)
      -- hq : ¬q          
      -- ⊢ r
                           -- If we look at h.2 (q ∨ r) and hq (¬q), it seems logical to deduce r.
                           -- After all, either q is true or r is true and we know q is false.
                           -- h.2 is a disjunction. We've dealt with these before.
      cases h.2 with       -- We consider cases for q ∨ r.
      | inl hq' =>         -- Now suppose q and label the hypothesis hq'. At this point, we have:

      -- hq : ¬q
      -- hq' : q
      -- ⊢ r
        contradiction      -- The contradiction tactic can be used to close any goal
                           -- when we have hypotheses that are opposed to each other (or False).
                           -- In this case, we have hq and hq'. These are directly opposed.
                           -- in classical logic, this is known as the principle of exposion.
                           -- Also known as ex falso quodlibet in latin, which means 
                           -- "from falsehood, anything follows".
                           -- In particular, "anything" refers to our goal in this case (r).

      | inr hr =>          -- Lastly, suppose r and label that hypothesis hr
        -- hr : r
        -- ⊢ r
        exact hr           -- Showing r is easy, we have hr.
```

Now, could we have also split on proposition `r`? Try this one yourself:
```lean
example (p q r : Prop) (h : p ∧ (q ∨ r)) : p ∧ q ∨ p ∧ r := by
  by_cases hr : r
  · -- case pos
    -- p q r : Prop
    -- h : p ∧ (q ∨ r)
    -- hr : r
    -- ⊢ p ∧ q ∨ p ∧ r
    right
    sorry
  · -- case neg
    -- p q r : Prop
    -- h : p ∧ (q ∨ r)
    -- hr : ¬r
    -- ⊢ p ∧ q ∨ p ∧ r
    left
    sorry
```
The proofs above required looking at another new tactic called `contradiction`. We'll look more into how this tactic can be used in a future chapter alongside negation.